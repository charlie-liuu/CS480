{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHZhkDjZEV-5"
      },
      "source": [
        "# CS 480/680 assignment 3\n",
        "\n",
        "Tips:\n",
        "- Please save a copy of this notebook to avoid losing your changes.\n",
        "- Debug your code and ensure that it can run.\n",
        "- Save the output of each cell. Failure to do so may result in your coding questions not being graded.\n",
        "- To accelerate the training time, you can choose 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' and set 'Hardware accelerator' to 'GPU'.\n",
        "- Your grade is independent of the accuracy of your models. Your grade will depend on the correctness of your code and implementation.\n",
        "\n",
        "Tips for sumbission:\n",
        "- Do not change the order of the problems.\n",
        "- Select 'Runtime' -> 'Run all' to run all cells and generate a final \"gradable\" version of your notebook and save your ipynb file.\n",
        "- Also use 'File' -> 'Print' and then print your report from your browser into a PDF file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK9gmfL4KxeF"
      },
      "source": [
        "## Question 1 - Implementing GAN for MNIST dataset (35 points)\n",
        "\n",
        "In this question we are going to impelement a generative network that can generate MNIST handwritten digits from random Gaussian noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n-Mg5YdE5YR"
      },
      "source": [
        "### Q1.1 (2 points)\n",
        "\n",
        "We start by creating the data loaders. Note that each MNIST image is of size 28\\*28. The **first task** is to build the ```data_loader```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-BU1m9IEXKY",
        "outputId": "e1aae1a6-3f14-4aa3-9b85-353ff6a40396"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "# Setting the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "batch_size = 100\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "#ToDo: Build the train data loader using the above batch_size varibale and shuffling the dataset.\n",
        "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_-NwpdfFGQs"
      },
      "source": [
        "### Q1.2 Generator (5 points)\n",
        "We will be building simple Generator and Discriminators that only consist of fully-connected layers. The Generator takes as input a Gaussian vector with dimension ```n_dim```. It consists of two linear layers with ```256``` and ```512``` nodes and the output layer should of dimension ```28*28=784``` so that it can be considered as unfolding a ```28*28``` image into a vector. The **second task** is to complete the following code for building the Generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uACYmlwwG2tL"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        #ToDo: Complete the code\n",
        "        #The activation function for the two hidden layers is ReLU.\n",
        "        #The last layer's activation function is Tanh.\n",
        "        self.fc1 = nn.Linear(n_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, 784)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #ToDo: Complete this function. The function takes as input x and outputs the result of applying the generator on x.\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.tanh(self.fc3(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InnZY0glITeK"
      },
      "source": [
        "### Q1.3 Discriminator (5 points)\n",
        "The Discriminator takes as input an image of size ```28*28```. It consists of two linear layers with ```512``` and ```256``` nodes and the output layer should be a single node. The **third task** complete the following code for building the Discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "t5hdsi10JWJI"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "            #ToDo: Complete the code\n",
        "            #The activation functions for the two hidden layers are ReLU.\n",
        "            #The last layer's activation function is Sigmoid.\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1) # Making sure that the batch of images has the shape [batch_size,28*28] instead of [batch_size,1,28,28]\n",
        "        #ToDo: Complete this function. The function takes as input x and outputs the result of applying the generator on x.\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXItGnRYK69p"
      },
      "source": [
        "### Q1.4 Initializaiton (2 points)\n",
        "\n",
        "The **fourth task** is to define the optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FyuaaCJLLCdE"
      },
      "outputs": [],
      "source": [
        "n_dim = 100\n",
        "generator = Generator(n_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "lr = 0.0002\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "#ToDo: define the two different Adam optimizers for generator and discriminator with learning rates of lr.\n",
        "\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASpkAhASZs-m"
      },
      "source": [
        "### Q1.5 Training (16 points)\n",
        "\n",
        "Note that when applying the ```criterion``` both the inputs should be on the same device. If you are defining a set of labels that you want to use in ```criterion``` you must move them to ```device``` first.\n",
        "\n",
        "Recall from the Goodfellow et al. 2014 that the following happens during the minimax game between Discriminator and the Generator.\n",
        "\n",
        "Discriminator is ascending\n",
        "$\\nabla\\frac{1}{m}\\sum_{i=1}^m\\log D(x^{(i)}) + \\log(1-D(G(z^{(i)})))$ where $D$ and $G$ denote the Districiminator and the Generator, respectively. Here, $x^{1},\\ldots,x^{m}$ is the batch of training examples (i.e., MNIST images) and $z^{(1)},\\ldots,z^{(m)}$ is a batch of random noise.\n",
        "\n",
        "Also note that for two batches $x=\\{x^{1},\\ldots,x^{m}\\}$ and $y=\\{y^{1},\\ldots,y^{m}\\}$ the BCELoss function $l(x,y)$ computes the following.\n",
        "$$l(x,y) = \\frac{1}{m}\\sum_{i=1}^m y^{(i)}\\log x^{(i)} + (1-y^{(i)})\\log (1- x^{(i)})$$\n",
        "\n",
        "The **fifth task** is to make sure that ```d_loss``` is computing the quantity that discriminator wants to ascend. This is achieved by setting ```d_loss = d_loss_real + d_loss_fake``` and letting ```d_loss_real``` to be $\\frac{1}{m}\\sum_{i=1}^m\\log D(x^{(i)})$ and ```d_loss_fake``` to be $\\frac{1}{m}\\sum_{i=1}^m \\log(1-D(G(z^{(i)})))$. Hint: ```d_loss_real``` is already implemented.\n",
        "\n",
        "We now turn to train the generator. Initially, the generator wants to descend the following $\\nabla\\frac{1}{m}\\sum_{i=1}^m \\log(1-D(G(z^{(i)})))$. However, this may affect the training of the generator specifically in the early stages of training where the discriminator can confidently reject the generated examples [Goodfellow et al., 2014]. Instead, we will require the generator to ascend $\\nabla\\frac{1}{m}\\sum_{i=1}^m \\log(D(G(z^{(i)})))$. The **sixth task** is to compute ```g_loss``` used for training the generator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA8t-sCQZ8SG",
        "outputId": "60af2e5b-4289-40a8-e30c-04bf2dc59170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [300/600], Discriminator Loss: 0.3335, Generator Loss: 3.6812\n",
            "Epoch [1/50], Step [600/600], Discriminator Loss: 0.1027, Generator Loss: 4.8390\n",
            "Epoch [2/50], Step [300/600], Discriminator Loss: 0.3501, Generator Loss: 3.7569\n",
            "Epoch [2/50], Step [600/600], Discriminator Loss: 0.2524, Generator Loss: 3.0594\n",
            "Epoch [3/50], Step [300/600], Discriminator Loss: 0.3659, Generator Loss: 3.6568\n",
            "Epoch [3/50], Step [600/600], Discriminator Loss: 0.5375, Generator Loss: 5.5310\n",
            "Epoch [4/50], Step [300/600], Discriminator Loss: 0.5706, Generator Loss: 1.5681\n",
            "Epoch [4/50], Step [600/600], Discriminator Loss: 0.1767, Generator Loss: 3.6159\n",
            "Epoch [5/50], Step [300/600], Discriminator Loss: 0.2325, Generator Loss: 3.6290\n",
            "Epoch [5/50], Step [600/600], Discriminator Loss: 0.3731, Generator Loss: 3.3580\n",
            "Epoch [6/50], Step [300/600], Discriminator Loss: 0.2926, Generator Loss: 4.1402\n",
            "Epoch [6/50], Step [600/600], Discriminator Loss: 0.8758, Generator Loss: 3.6118\n",
            "Epoch [7/50], Step [300/600], Discriminator Loss: 0.3631, Generator Loss: 3.3856\n",
            "Epoch [7/50], Step [600/600], Discriminator Loss: 0.5101, Generator Loss: 4.3404\n",
            "Epoch [8/50], Step [300/600], Discriminator Loss: 0.3288, Generator Loss: 3.3152\n",
            "Epoch [8/50], Step [600/600], Discriminator Loss: 0.6702, Generator Loss: 2.0801\n",
            "Epoch [9/50], Step [300/600], Discriminator Loss: 0.3970, Generator Loss: 3.1726\n",
            "Epoch [9/50], Step [600/600], Discriminator Loss: 0.5824, Generator Loss: 2.4466\n",
            "Epoch [10/50], Step [300/600], Discriminator Loss: 0.2649, Generator Loss: 3.6634\n",
            "Epoch [10/50], Step [600/600], Discriminator Loss: 0.3071, Generator Loss: 4.2688\n",
            "Epoch [11/50], Step [300/600], Discriminator Loss: 0.6675, Generator Loss: 3.4580\n",
            "Epoch [11/50], Step [600/600], Discriminator Loss: 0.1587, Generator Loss: 5.2084\n",
            "Epoch [12/50], Step [300/600], Discriminator Loss: 0.5538, Generator Loss: 2.5026\n",
            "Epoch [12/50], Step [600/600], Discriminator Loss: 0.2156, Generator Loss: 3.6668\n",
            "Epoch [13/50], Step [300/600], Discriminator Loss: 0.1928, Generator Loss: 3.6749\n",
            "Epoch [13/50], Step [600/600], Discriminator Loss: 0.2469, Generator Loss: 4.3576\n",
            "Epoch [14/50], Step [300/600], Discriminator Loss: 0.3387, Generator Loss: 5.5736\n",
            "Epoch [14/50], Step [600/600], Discriminator Loss: 0.3490, Generator Loss: 3.7197\n",
            "Epoch [15/50], Step [300/600], Discriminator Loss: 0.2809, Generator Loss: 4.2669\n",
            "Epoch [15/50], Step [600/600], Discriminator Loss: 0.5877, Generator Loss: 2.8588\n",
            "Epoch [16/50], Step [300/600], Discriminator Loss: 0.3608, Generator Loss: 4.3082\n",
            "Epoch [16/50], Step [600/600], Discriminator Loss: 0.2256, Generator Loss: 3.2752\n",
            "Epoch [17/50], Step [300/600], Discriminator Loss: 0.4629, Generator Loss: 2.9334\n",
            "Epoch [17/50], Step [600/600], Discriminator Loss: 0.3223, Generator Loss: 4.6993\n",
            "Epoch [18/50], Step [300/600], Discriminator Loss: 0.3273, Generator Loss: 4.7904\n",
            "Epoch [18/50], Step [600/600], Discriminator Loss: 0.3473, Generator Loss: 4.3883\n",
            "Epoch [19/50], Step [300/600], Discriminator Loss: 0.3601, Generator Loss: 2.9439\n",
            "Epoch [19/50], Step [600/600], Discriminator Loss: 0.3277, Generator Loss: 2.7880\n",
            "Epoch [20/50], Step [300/600], Discriminator Loss: 0.2611, Generator Loss: 4.0649\n",
            "Epoch [20/50], Step [600/600], Discriminator Loss: 0.3187, Generator Loss: 3.5144\n",
            "Epoch [21/50], Step [300/600], Discriminator Loss: 0.4184, Generator Loss: 3.7874\n",
            "Epoch [21/50], Step [600/600], Discriminator Loss: 0.2735, Generator Loss: 2.9293\n",
            "Epoch [22/50], Step [300/600], Discriminator Loss: 0.6712, Generator Loss: 2.5092\n",
            "Epoch [22/50], Step [600/600], Discriminator Loss: 0.4231, Generator Loss: 2.3430\n",
            "Epoch [23/50], Step [300/600], Discriminator Loss: 0.4056, Generator Loss: 2.6115\n",
            "Epoch [23/50], Step [600/600], Discriminator Loss: 0.4669, Generator Loss: 3.0581\n",
            "Epoch [24/50], Step [300/600], Discriminator Loss: 0.2791, Generator Loss: 3.3116\n",
            "Epoch [24/50], Step [600/600], Discriminator Loss: 0.3980, Generator Loss: 2.9884\n",
            "Epoch [25/50], Step [300/600], Discriminator Loss: 0.4332, Generator Loss: 2.4413\n",
            "Epoch [25/50], Step [600/600], Discriminator Loss: 0.7973, Generator Loss: 3.1649\n",
            "Epoch [26/50], Step [300/600], Discriminator Loss: 0.6571, Generator Loss: 3.2435\n",
            "Epoch [26/50], Step [600/600], Discriminator Loss: 0.2555, Generator Loss: 3.8158\n",
            "Epoch [27/50], Step [300/600], Discriminator Loss: 0.3381, Generator Loss: 2.5305\n",
            "Epoch [27/50], Step [600/600], Discriminator Loss: 0.4714, Generator Loss: 2.3913\n",
            "Epoch [28/50], Step [300/600], Discriminator Loss: 0.3856, Generator Loss: 2.3230\n",
            "Epoch [28/50], Step [600/600], Discriminator Loss: 0.3886, Generator Loss: 3.0849\n",
            "Epoch [29/50], Step [300/600], Discriminator Loss: 0.9624, Generator Loss: 2.7955\n",
            "Epoch [29/50], Step [600/600], Discriminator Loss: 0.4462, Generator Loss: 2.6558\n",
            "Epoch [30/50], Step [300/600], Discriminator Loss: 0.3991, Generator Loss: 2.4716\n",
            "Epoch [30/50], Step [600/600], Discriminator Loss: 0.4944, Generator Loss: 3.6562\n",
            "Epoch [31/50], Step [300/600], Discriminator Loss: 0.5154, Generator Loss: 3.6057\n",
            "Epoch [31/50], Step [600/600], Discriminator Loss: 0.5074, Generator Loss: 2.2239\n",
            "Epoch [32/50], Step [300/600], Discriminator Loss: 0.4636, Generator Loss: 2.8874\n",
            "Epoch [32/50], Step [600/600], Discriminator Loss: 0.5913, Generator Loss: 2.2005\n",
            "Epoch [33/50], Step [300/600], Discriminator Loss: 0.4659, Generator Loss: 2.7781\n",
            "Epoch [33/50], Step [600/600], Discriminator Loss: 0.6279, Generator Loss: 3.6367\n",
            "Epoch [34/50], Step [300/600], Discriminator Loss: 0.6230, Generator Loss: 2.4554\n",
            "Epoch [34/50], Step [600/600], Discriminator Loss: 0.4555, Generator Loss: 2.3664\n",
            "Epoch [35/50], Step [300/600], Discriminator Loss: 0.5538, Generator Loss: 3.0832\n",
            "Epoch [35/50], Step [600/600], Discriminator Loss: 0.4998, Generator Loss: 3.6377\n",
            "Epoch [36/50], Step [300/600], Discriminator Loss: 0.5748, Generator Loss: 2.3442\n",
            "Epoch [36/50], Step [600/600], Discriminator Loss: 0.4514, Generator Loss: 2.9272\n",
            "Epoch [37/50], Step [300/600], Discriminator Loss: 0.5289, Generator Loss: 3.0431\n",
            "Epoch [37/50], Step [600/600], Discriminator Loss: 0.3986, Generator Loss: 3.6503\n",
            "Epoch [38/50], Step [300/600], Discriminator Loss: 0.3958, Generator Loss: 3.3372\n",
            "Epoch [38/50], Step [600/600], Discriminator Loss: 0.7108, Generator Loss: 3.1412\n",
            "Epoch [39/50], Step [300/600], Discriminator Loss: 0.7069, Generator Loss: 2.1862\n",
            "Epoch [39/50], Step [600/600], Discriminator Loss: 0.5644, Generator Loss: 2.3280\n",
            "Epoch [40/50], Step [300/600], Discriminator Loss: 0.4767, Generator Loss: 2.4648\n",
            "Epoch [40/50], Step [600/600], Discriminator Loss: 0.4992, Generator Loss: 2.3911\n",
            "Epoch [41/50], Step [300/600], Discriminator Loss: 0.6331, Generator Loss: 2.5203\n",
            "Epoch [41/50], Step [600/600], Discriminator Loss: 0.3557, Generator Loss: 2.9844\n",
            "Epoch [42/50], Step [300/600], Discriminator Loss: 0.5430, Generator Loss: 2.0119\n",
            "Epoch [42/50], Step [600/600], Discriminator Loss: 0.7083, Generator Loss: 1.8532\n",
            "Epoch [43/50], Step [300/600], Discriminator Loss: 0.5930, Generator Loss: 2.4707\n",
            "Epoch [43/50], Step [600/600], Discriminator Loss: 0.7647, Generator Loss: 2.0789\n",
            "Epoch [44/50], Step [300/600], Discriminator Loss: 0.6751, Generator Loss: 2.1913\n",
            "Epoch [44/50], Step [600/600], Discriminator Loss: 0.4967, Generator Loss: 3.5449\n",
            "Epoch [45/50], Step [300/600], Discriminator Loss: 0.5454, Generator Loss: 2.2827\n",
            "Epoch [45/50], Step [600/600], Discriminator Loss: 0.5808, Generator Loss: 2.1816\n",
            "Epoch [46/50], Step [300/600], Discriminator Loss: 0.6068, Generator Loss: 1.8848\n",
            "Epoch [46/50], Step [600/600], Discriminator Loss: 0.5502, Generator Loss: 1.5733\n",
            "Epoch [47/50], Step [300/600], Discriminator Loss: 0.5996, Generator Loss: 1.8172\n",
            "Epoch [47/50], Step [600/600], Discriminator Loss: 0.5425, Generator Loss: 2.7223\n",
            "Epoch [48/50], Step [300/600], Discriminator Loss: 0.5508, Generator Loss: 2.1439\n",
            "Epoch [48/50], Step [600/600], Discriminator Loss: 0.7645, Generator Loss: 2.8454\n",
            "Epoch [49/50], Step [300/600], Discriminator Loss: 0.7220, Generator Loss: 2.4764\n",
            "Epoch [49/50], Step [600/600], Discriminator Loss: 0.7164, Generator Loss: 2.4832\n",
            "Epoch [50/50], Step [300/600], Discriminator Loss: 0.3355, Generator Loss: 2.5979\n",
            "Epoch [50/50], Step [600/600], Discriminator Loss: 0.7950, Generator Loss: 1.7600\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "total_step = len(data_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        real_images = images.to(device)\n",
        "\n",
        "        # Training discriminator\n",
        "        discriminator.zero_grad()\n",
        "        real_outputs = discriminator(real_images)\n",
        "        real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
        "        d_loss_real = criterion(real_outputs, real_labels)\n",
        "\n",
        "        #ToDo: Compute d_loss_fake\n",
        "        z = torch.randn(batch_size, n_dim).to(device)\n",
        "        fake_images = generator(z)\n",
        "        fake_outputs = discriminator(fake_images)\n",
        "        fake_labels = torch.zeros(fake_images.size(0), 1).to(device)\n",
        "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
        "        fake_score = fake_outputs\n",
        "\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Training generator\n",
        "        generator.zero_grad()\n",
        "\n",
        "        #ToDo: compute g_loss\n",
        "        z = torch.randn(batch_size, n_dim).to(device)\n",
        "        fake_images = generator(z)\n",
        "        outputs = discriminator(fake_images)\n",
        "        real_labels = torch.ones(fake_images.size(0), 1).to(device)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if (i+1) % 300 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], '\n",
        "                  f'Discriminator Loss: {d_loss.item():.4f}, '\n",
        "                  f'Generator Loss: {g_loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3CGaQzDDRg8"
      },
      "source": [
        "### Q1.6 Plotting the results (5 points)\n",
        "The **seventh and final task** for this question is to try the trained generator. Generate 4 random vectors, apply the generator on them, and plot the resulting image in a single 2 by 2 plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "lvUJnaqSJCPM",
        "outputId": "9cc32594-0edd-4746-ab6b-d1441155a846"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rand_vec = torch.randn(4, n_dim).to(device)\n",
        "with torch.no_grad():\n",
        "  fake_images = generator(rand_vec)\n",
        "  fake_images = fake_images.view(-1, 28, 28).cpu().detach().numpy()\n",
        "\n",
        "# Create a 2x2 grid plot for the generated images\n",
        "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        axes[i, j].imshow(fake_images[i * 2 + j], cmap='gray')\n",
        "        axes[i, j].axis('off')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifoL2LfccMKA"
      },
      "source": [
        "## Question 2 - Transformers (65 points)\n",
        "In this question we will be implementing a transformer architecture and apply it on a simple artificially generated dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtZX21CEcu6p"
      },
      "source": [
        "### Q2.1 Multi-Head Attention (9 points)\n",
        "\n",
        "The attention layer works on batches of training data where each training data itself is a sequence of words/token with length at most ```seq_length``` and each word is embedded into a ```d_embedding``` dimensional vector, e.g., later when we run the transformer on the artificial dataset, the inputs to the transformer and attention layer have shapes of ```[batch_size,seq_length,d_embedding]```. Recall from the lecture that a multi-head attention layer consists of ```n_heads``` self-attention heads. The input matrices Query Q, Key K, and Value V to multi-attention head are passed through linear layers $W_Q$, $W_K$, and $W_V$ which are trainable parameters. The multi-head attention will divide the embedding vector of the input into ```n_heads``` vector of size ```d_k``` and use each as an attention head. This is particularly done by the ```make_heads``` function which transforms the input to an output of shape ```[batch_size, n_heads, seq_length, d_k]```. The multi-head attention then computes the probabilities using the input values, which is implemented by the ```compute_attention_probs``` function. It then joins them using the ```join_heads``` function and finally passes them through an output linear layer $W_O$. The **first task** is to implement the ```forward``` function of Multi-Head attention using the functions already implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pE_j8e5u4aLv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_embedding % n_heads == 0\n",
        "\n",
        "        self.d_embedding = d_embedding\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_embedding // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_k = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_v = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_o = nn.Linear(d_embedding, d_embedding)\n",
        "\n",
        "    def compute_attention_probs(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def make_heads(self, x):\n",
        "        batch_size, seq_length, d_embedding = x.size()\n",
        "        y=x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def join_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_embedding)\n",
        "\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "       #ToDo: Complete code\n",
        "      Q_proj = self.W_q(Q)\n",
        "      K_proj = self.W_k(K)\n",
        "      V_proj = self.W_v(V)\n",
        "\n",
        "      # Make heads\n",
        "      Q_heads = self.make_heads(Q_proj)\n",
        "      K_heads = self.make_heads(K_proj)\n",
        "      V_heads = self.make_heads(V_proj)\n",
        "\n",
        "      # Compute attention probabilities\n",
        "      attention_output = self.compute_attention_probs(Q_heads, K_heads, V_heads, mask)\n",
        "      joined_output = self.join_heads(attention_output)\n",
        "      output = self.W_o(joined_output)\n",
        "\n",
        "      return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT0KE6fB6497"
      },
      "source": [
        "### Q2.2 Feed-forward layer (2 points)\n",
        "The output of the multi-head attention layer is used as an input to the feed-forward layer in the encoder. This layer is implemented in the following class. The **second task** is to complete the ```forward``` function of this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "InGgN9XN7muD"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_embedding, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_embedding, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_embedding)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #ToDO: Complete code\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaZ1PCFJ9DXf"
      },
      "source": [
        "### Q2.3 Encoder Block (9 points)\n",
        "\n",
        "We now have all the necessary classes to build the Encoder block. The encoder block gets as input a batch of data. It first passes the input through the multi-attention layer. It then applies 1) a droput layer, 2) adds the input to the output of droupout layer as a residual, 3) applies a layer normalization, 4) and passes the output to the feed-forward layer. Finally, it applies 1) a droput layer, 2) adds the input to the output of droupout layer as a residual, and 3) Applies a layer normalization. The **third task** is to complete the ```forward``` function of this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "s0MnjetG_Zum"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads, d_ff, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.feed_forward = FeedForward(d_embedding, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_embedding)\n",
        "        self.norm2 = nn.LayerNorm(d_embedding)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #ToDo: Complete code\n",
        "        \"\"\"\n",
        "        Returns: The output of encoder on a given input batch x\n",
        "        \"\"\"\n",
        "        atten = self.self_attention(x,x,x, mask)\n",
        "        x = x + self.dropout(atten)\n",
        "        x = self.norm1(x)\n",
        "        x = self.feed_forward(x)\n",
        "\n",
        "        #adding resid again\n",
        "        x = x + self.dropout(x)\n",
        "        x = self.norm2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jru9E9dZ_aKv"
      },
      "source": [
        "### Q2.4 Decoder Block (20 points)\n",
        "\n",
        "In this part we will build the decoder block. The **fourth task** is to implement the DecoderBlock. You can refer to the lecture notes to find the architecture of the decoder block including the multi-attention layers, add and norm layers, and the feedforward layer. You can also add a *dropout* layer to the final add and norm layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HCX_D3xoFYJo"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           self_attention: decoder's multi-head attention\n",
        "           cross_attention: The multi-head attention layer between the encoder and the decoder\n",
        "           feed_forward: feed-forward layer\n",
        "           mask: mask to be given for multi head attention\n",
        "           norm1: First Normalization layer\n",
        "           norm2: Second Normalization layer\n",
        "           norm3: Third Normalization layer\n",
        "           dropout: Final dropout layer\n",
        "        \"\"\"\n",
        "        self.self_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.cross_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.feed_forward = FeedForward(d_embedding, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_embedding)\n",
        "        self.norm2 = nn.LayerNorm(d_embedding)\n",
        "        self.norm3 = nn.LayerNorm(d_embedding)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Returns: The output of decoder on a given input batch x\n",
        "        \"\"\"\n",
        "\n",
        "        #decoder attention\n",
        "        attention_output = self.self_attention(x,x,x, tgt_mask)\n",
        "        x = x + self.dropout(attention_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        #cross attention\n",
        "        cross_output = self.cross_attention(x,enc_output,enc_output, src_mask)\n",
        "        x = x + self.dropout(cross_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "\n",
        "        #feed-forward\n",
        "        feed_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(feed_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyH9ROxRI252"
      },
      "source": [
        "### Q2.5 Transformer Block (20 points)\n",
        "\n",
        "Finally, we use the classes defined for EncoderBlock and DecoderBlock to build the TransformerBlock. The **fifth task** is to complete the ```forward``` function of this class. Note that ```num_layers``` indicates the number of encoder and decoder layers in the transformer. You can refer to the final architecture of the Transformer from the lectures. You can also include a dropout layer after embedding the source and target inputs. Remember to apply positional encoding to the input batch before passing it to the transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2Sz2JAuwmnNn"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_embedding, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_embedding)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_embedding, 2).float() * -(math.log(10000.0) / d_embedding))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_embedding, n_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_embedding)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_embedding)\n",
        "        self.positional_encoding = PositionalEncoding(d_embedding, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderBlock(d_embedding, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderBlock(d_embedding, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_embedding, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Masking the input; the outputs returned by this function should be passed to encoder and decoder layers\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        #ToDo: complete the code\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        \"\"\"\n",
        "        Returns: The output of transformer on a given input batch x\n",
        "        \"\"\"\n",
        "        src_embedded = self.encoder_embedding(src)\n",
        "        src_embedded = self.positional_encoding(src_embedded)\n",
        "\n",
        "        encoder_output = src_embedded\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_output = encoder_layer(encoder_output, src_mask)\n",
        "\n",
        "        tgt_embedded = self.decoder_embedding(tgt)\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
        "\n",
        "\n",
        "        decoder_output = tgt_embedded\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_output = decoder_layer(decoder_output, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(self.dropout(decoder_output))\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGtuTlqWms4B"
      },
      "source": [
        "### Q2.6 Training (5 points)\n",
        "We can now train the model on an imaginary randomly generated dataset. The **sixth task** is to complete the training code and plot the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94xoG6QF5gnv",
        "outputId": "41714a79-35bd-4dd6-8e35-9e0abeda00c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Loss: 8.6991 \n",
            "Epoch [2/25], Loss: 8.5934 \n",
            "Epoch [3/25], Loss: 8.5271 \n",
            "Epoch [4/25], Loss: 8.4771 \n",
            "Epoch [5/25], Loss: 8.4330 \n",
            "Epoch [6/25], Loss: 8.3841 \n",
            "Epoch [7/25], Loss: 8.3298 \n",
            "Epoch [8/25], Loss: 8.2679 \n",
            "Epoch [9/25], Loss: 8.2058 \n",
            "Epoch [10/25], Loss: 8.1320 \n",
            "Epoch [11/25], Loss: 8.0658 \n",
            "Epoch [12/25], Loss: 8.0053 \n",
            "Epoch [13/25], Loss: 7.9380 \n",
            "Epoch [14/25], Loss: 7.8679 \n",
            "Epoch [15/25], Loss: 7.7991 \n",
            "Epoch [16/25], Loss: 7.7157 \n",
            "Epoch [17/25], Loss: 7.6458 \n",
            "Epoch [18/25], Loss: 7.5762 \n",
            "Epoch [19/25], Loss: 7.4988 \n",
            "Epoch [20/25], Loss: 7.4174 \n",
            "Epoch [21/25], Loss: 7.3456 \n",
            "Epoch [22/25], Loss: 7.2677 \n",
            "Epoch [23/25], Loss: 7.1898 \n",
            "Epoch [24/25], Loss: 7.1191 \n",
            "Epoch [25/25], Loss: 7.0398 \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_embedding = 512\n",
        "n_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "nums_epoch =25\n",
        "loss_list = []\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_embedding, n_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(nums_epoch):\n",
        "   #ToDo: complete the code\n",
        "   transformer_out = transformer(src_data, tgt_data[:,:-1])\n",
        "   # Compute loss\n",
        "   loss = criterion(transformer_out.view(-1, tgt_vocab_size), tgt_data[:,1:].contiguous().view(-1))\n",
        "   optimizer.zero_grad()\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "   loss_list.append(loss.item())\n",
        "   print(f'Epoch [{epoch+1}/{nums_epoch}], Loss: {loss.item():.4f} ')\n",
        "   \n",
        "plt.plot(loss_list)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
